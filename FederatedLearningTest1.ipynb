{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjwTRL5xvlaU+5hzubsLS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marinasjp/Federated-Learning/blob/main/FederatedLearningTest1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade tensorflow-model-optimization\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install --quiet tensorflow-federated==0.20.0\n",
        "\n",
        "import pandas as pd\n",
        "import collections\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from typing import Callable, Sequence, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "metadata": {
        "id": "qD1RUHQiDJb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf241c9-8201-4469-dc3e-bfdd988c81a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (22.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.24.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.34.0 Requires-Python ~=3.9.0; 0.36.0 Requires-Python ~=3.9.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement jaxlib~=0.1.76 (from tensorflow-federated) (from versions: 0.3.14, 0.3.15, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13, 0.4.14, 0.4.15, 0.4.16, 0.4.17, 0.4.18, 0.4.19)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for jaxlib~=0.1.76\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets"
      ],
      "metadata": {
        "id": "7itnAdgHcSkW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (train_images, train_labels), (test_images, test_labels) =datasets.cifar10.load_data()\n",
        "# (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
        "# # Normalize pixel values to be between 0 and 1\n",
        "# train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "#     (train_images, train_labels)).shuffle(10000).batch(32)\n",
        "\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WbCnEoqKa3B",
        "outputId": "23a18ad2-28d0-4caa-d48b-9871752b25c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dirichlet_split(train_idcs, train_labels, alpha, n_clients):\n",
        "   '''\n",
        "    Splits a list of data indices with corresponding labels\n",
        "    into subsets according to a dirichlet distribution with parameter\n",
        "    alpha\n",
        "    '''\n",
        "    n_classes = train_labels.max()+1\n",
        "    label_distribution = np.random.dirichlet([alpha]*n_clients, n_classes)\n",
        "\n",
        "    class_idcs = [np.argwhere(train_labels[train_idcs]==y).flatten()\n",
        "           for y in range(n_classes)]\n",
        "\n",
        "    client_idcs = [[] for _ in range(n_clients)]\n",
        "    for c, fracs in zip(class_idcs, label_distribution):\n",
        "        for i, idcs in enumerate(np.split(c, (np.cumsum(fracs)[:-1]*len(c)).astype(int))):\n",
        "            client_idcs[i] += [idcs]\n",
        "\n",
        "    client_idcs = [train_idcs[np.concatenate(idcs)] for idcs in client_idcs]\n",
        "\n",
        "    return client_idcs\n"
      ],
      "metadata": {
        "id": "M8sr9Wxl2FU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_SHAPE = (32, 32, 3)\n",
        "TOTAL_FEATURE_SIZE = 32 * 32 * 3\n",
        "NUM_CLASSES = 10\n",
        "TRAIN_EXAMPLES = 50000\n",
        "TEST_EXAMPLES = 10000\n",
        "# Number of training examples per class: 50,000 / 10.\n",
        "TRAIN_EXAMPLES_PER_LABEL = 5000\n",
        "# Number of test examples per class: 10,000 / 10.\n",
        "TEST_EXAMPLES_PER_LABEL = 1000\n",
        "\n",
        "\n",
        "def load_cifar10_federated(\n",
        "    dirichlet_parameter: float = 1,\n",
        "    num_clients: int = 10,\n",
        ") -> Tuple[tff.simulation.datasets.ClientData,\n",
        "           tff.simulation.datasets.ClientData]:\n",
        "  \"\"\"Construct a federated dataset from the centralized CIFAR-10.\n",
        "\n",
        "  Sampling based on Dirichlet distribution over categories, following the paper\n",
        "  Measuring the Effects of Non-Identical Data Distribution for\n",
        "  Federated Visual Classification (https://arxiv.org/abs/1909.06335).\n",
        "\n",
        "  Args:\n",
        "    dirichlet_parameter: Parameter of Dirichlet distribution. Each client\n",
        "      samples from this Dirichlet to get a multinomial distribution over\n",
        "      classes. It controls the data heterogeneity of clients. If approaches 0,\n",
        "      then each client only have data from a single category label. If\n",
        "      approaches infinity, then the client distribution will approach IID\n",
        "      partitioning.\n",
        "    num_clients: The number of clients the examples are going to be partitioned\n",
        "      on.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of `tff.simulation.datasets.ClientData` representing unpreprocessed\n",
        "    train data and test data.\n",
        "  \"\"\"\n",
        "  train_images, train_labels = tensorflow_datasets.as_numpy(\n",
        "      tensorflow_datasets.load(\n",
        "          name='cifar10',\n",
        "          split='train',\n",
        "          batch_size=-1,\n",
        "          as_supervised=True,\n",
        "      ))\n",
        "  test_images, test_labels = tensorflow_datasets.as_numpy(\n",
        "      tensorflow_datasets.load(\n",
        "          name='cifar10',\n",
        "          split='test',\n",
        "          batch_size=-1,\n",
        "          as_supervised=True,\n",
        "      ))\n",
        "  train_clients = collections.OrderedDict()\n",
        "  test_clients = collections.OrderedDict()\n",
        "\n",
        "  train_multinomial_vals = []\n",
        "  test_multinomial_vals = []\n",
        "  # Each client has a multinomial distribution over classes drawn from a\n",
        "  # Dirichlet.\n",
        "  for i in range(num_clients):\n",
        "    proportion = np.random.dirichlet(dirichlet_parameter *\n",
        "                                     np.ones(NUM_CLASSES,))\n",
        "    train_multinomial_vals.append(proportion)\n",
        "    test_multinomial_vals.append(proportion)\n",
        "\n",
        "  train_multinomial_vals = np.array(train_multinomial_vals)\n",
        "  test_multinomial_vals = np.array(test_multinomial_vals)\n",
        "\n",
        "  train_example_indices = []\n",
        "  test_indices = []\n",
        "  for k in range(NUM_CLASSES):\n",
        "    train_label_k = np.where(train_labels == k)[0]\n",
        "    np.random.shuffle(train_label_k)\n",
        "    train_example_indices.append(train_label_k)\n",
        "    test_label_k = np.where(test_labels == k)[0]\n",
        "    np.random.shuffle(test_label_k)\n",
        "    test_indices.append(test_label_k)\n",
        "\n",
        "  train_example_indices = np.array(train_example_indices)\n",
        "  test_indices = np.array(test_indices)\n",
        "\n",
        "  train_client_samples = [[] for _ in range(num_clients)]\n",
        "  test_client_samples = [[] for _ in range(num_clients)]\n",
        "  train_count = np.zeros(NUM_CLASSES).astype(int)\n",
        "  test_count = np.zeros(NUM_CLASSES).astype(int)\n",
        "\n",
        "  train_examples_per_client = int(TRAIN_EXAMPLES / num_clients)\n",
        "  test_examples_per_client = int(TEST_EXAMPLES / num_clients)\n",
        "  for k in range(num_clients):\n",
        "\n",
        "    for i in range(train_examples_per_client):\n",
        "      sampled_label = np.argwhere(\n",
        "          np.random.multinomial(1, train_multinomial_vals[k, :]) == 1)[0][0]\n",
        "      train_client_samples[k].append(\n",
        "          train_example_indices[sampled_label, train_count[sampled_label]])\n",
        "      train_count[sampled_label] += 1\n",
        "      if train_count[sampled_label] == TRAIN_EXAMPLES_PER_LABEL:\n",
        "        train_multinomial_vals[:, sampled_label] = 0\n",
        "        train_multinomial_vals = (\n",
        "            train_multinomial_vals /\n",
        "            train_multinomial_vals.sum(axis=1)[:, None])\n",
        "\n",
        "    for i in range(test_examples_per_client):\n",
        "      sampled_label = np.argwhere(\n",
        "          np.random.multinomial(1, test_multinomial_vals[k, :]) == 1)[0][0]\n",
        "      test_client_samples[k].append(test_indices[sampled_label,\n",
        "                                                 test_count[sampled_label]])\n",
        "      test_count[sampled_label] += 1\n",
        "      if test_count[sampled_label] == TEST_EXAMPLES_PER_LABEL:\n",
        "        test_multinomial_vals[:, sampled_label] = 0\n",
        "        test_multinomial_vals = (\n",
        "            test_multinomial_vals / test_multinomial_vals.sum(axis=1)[:, None])\n",
        "\n",
        "  for i in range(num_clients):\n",
        "    client_name = str(i)\n",
        "    x_train = train_images[np.array(train_client_samples[i])]\n",
        "    y_train = train_labels[np.array(\n",
        "        train_client_samples[i])].astype('int64').squeeze()\n",
        "    train_data = collections.OrderedDict(\n",
        "        (('image', x_train), ('label', y_train)))\n",
        "    train_clients[client_name] = train_data\n",
        "\n",
        "    x_test = test_images[np.array(test_client_samples[i])]\n",
        "    y_test = test_labels[np.array(\n",
        "        test_client_samples[i])].astype('int64').squeeze()\n",
        "    test_data = collections.OrderedDict((('image', x_test), ('label', y_test)))\n",
        "    test_clients[client_name] = test_data\n",
        "\n",
        "  train_dataset = tff.simulation.datasets.ClientData(train_clients)\n",
        "  test_dataset = tff.simulation.datasets.ClientData(test_clients)\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "train_dataset, test_dataset = load_cifar10_federated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjsVhlunZ48t",
        "outputId": "762f8499-96d9-4903-b1a7-780ebc394467"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0458a0ac0ed6>:98: RuntimeWarning: invalid value encountered in divide\n",
            "  train_multinomial_vals /\n",
            "<ipython-input-16-0458a0ac0ed6>:110: RuntimeWarning: invalid value encountered in divide\n",
            "  test_multinomial_vals / test_multinomial_vals.sum(axis=1)[:, None])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2MbrCmccsK_",
        "outputId": "6640d5dd-5a7e-4edd-dd38-9a5480fec179"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow_federated.python.simulation.datasets.from_tensor_slices_client_data.TestClientData object at 0x7f2a6cda3340>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "num_classes = 10\n",
        "EPOCHS = 15\n",
        "NUM_CLIENTS = 100\n",
        "\n",
        "# client_datasets = []\n",
        "# for i in range(NUM_CLIENTS):\n",
        "#     # Randomly sample a subset of the dataset.\n",
        "#     client_dataset_x = train_images[tf.random.shuffle(tf.range(len(train_images))), :]\n",
        "#     client_dataset_y = train_labels[tf.random.shuffle(tf.range(len(train_labels))), :]\n",
        "\n",
        "#     # Create a TensorFlow dataset for the client.\n",
        "#     client_dataset = emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[i])\n",
        "#     # client_dataset = tf.data.Dataset.from_tensor_slices((client_dataset_x, client_dataset_y))\n",
        "\n",
        "#     # Add the client dataset to the list of client datasets.\n",
        "#     client_datasets.append(client_dataset)\n",
        "\n",
        "\n",
        "# def preprocess(dataset):\n",
        "\n",
        "#   def batch_format_fn(element):\n",
        "#     \"\"\"Flatten a batch of EMNIST data and return a (features, label) tuple.\"\"\"\n",
        "#     return (tf.reshape(element['pixels'], [-1, 784]),\n",
        "#             tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "#   return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n",
        "\n",
        "# client_ids = sorted(train_images.client_ids)[:NUM_CLIENTS]\n",
        "# federated_train_data = [preprocess(train_images.create_tf_dataset_for_client(x))\n",
        "#   for x in client_ids\n",
        "# ]"
      ],
      "metadata": {
        "id": "cbt-N5M6SreB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlr4n-QHCjVm"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, padding='same', input_shape=train_images.shape[1:], activation='relu'),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    # tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    # tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    # tf.keras.layers.MaxPooling2D(),\n",
        "    # tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(512, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.48),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "  ])\n",
        "\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=federated_train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "\n",
        "whimsy_model = model_fn()\n",
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  model = model_fn()\n",
        "  return model.trainable_variables\n",
        "@tff.federated_computation\n",
        "def initialize_fn():\n",
        "  return tff.federated_value(server_init(), tff.SERVER)\n",
        "\n",
        "tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
        "\n",
        "model_weights_type = server_init.type_signature.result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@tf.function\n",
        "def client_update(model, dataset, server_weights, client_optimizer):\n",
        "  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
        "  # Initialize the client model with the current server weights.\n",
        "  client_weights = model.trainable_variables\n",
        "  # Assign the server weights to the client model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        client_weights, server_weights)\n",
        "\n",
        "  # Use the client_optimizer to update the local model.\n",
        "  for batch in dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Compute a forward pass on the batch of data\n",
        "      outputs = model.forward_pass(batch)\n",
        "\n",
        "    # Compute the corresponding gradient\n",
        "    grads = tape.gradient(outputs.loss, client_weights)\n",
        "    grads_and_vars = zip(grads, client_weights)\n",
        "\n",
        "    # Apply the gradient using a client optimizer.\n",
        "    client_optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "  return client_weights\n",
        "\n",
        "# tff.tf_computation accepts a client datasets and server weights, and outputs\n",
        "# an updated client weights tensor.\n",
        "@tff.tf_computation(tf_dataset_type, model_weights_type)\n",
        "def client_update_fn(tf_dataset, server_weights):\n",
        "  model = model_fn()\n",
        "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "  return client_update(model, tf_dataset, server_weights, client_optimizer)"
      ],
      "metadata": {
        "id": "GvIY1d85LJQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@tf.function\n",
        "def server_update(model, mean_client_weights):\n",
        "  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
        "  model_weights = model.trainable_variables\n",
        "  # Assign the mean client weights to the server model.\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        model_weights, mean_client_weights)\n",
        "  return model_weights\n",
        "\n",
        "@tff.tf_computation(model_weights_type)\n",
        "def server_update_fn(mean_client_weights):\n",
        "  model = model_fn()\n",
        "  return server_update(model, mean_client_weights)\n"
      ],
      "metadata": {
        "id": "P7aZuDGdLJ-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
        "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
        "\n",
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  model = model_fn()\n",
        "  return model.trainable_variables\n",
        "\n",
        "\n",
        "@tff.federated_computation\n",
        "def initialize_fn():\n",
        "  return tff.federated_value(server_init(), tff.SERVER)\n"
      ],
      "metadata": {
        "id": "NfqsL0BFLVPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
        "def next_fn(server_weights, federated_dataset):\n",
        "  # Broadcast the server weights to the clients.\n",
        "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
        "\n",
        "  # Each client computes their updated weights.\n",
        "  client_weights = tff.federated_map(client_update_fn, (federated_dataset, server_weights_at_client))\n",
        "\n",
        "  # The server averages these updates.\n",
        "  mean_client_weights = tff.federated_mean(client_weights)\n",
        "\n",
        "  # The server updates its model.\n",
        "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
        "\n",
        "  return server_weights\n"
      ],
      "metadata": {
        "id": "5rTH7GGaHvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "federated_algorithm = tff.templates.IterativeProcess(\n",
        "    initialize_fn=initialize_fn,\n",
        "    next_fn=next_fn\n",
        ")\n",
        "\n",
        "central_emnist_test = emnist_test.create_tf_dataset_from_all_clients()\n",
        "central_emnist_test = preprocess(central_emnist_test)\n",
        "\n",
        "def evaluate(server_state):\n",
        "  keras_model = create_keras_model()\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "  )\n",
        "  keras_model.set_weights(server_state)\n",
        "  keras_model.evaluate(central_emnist_test)\n",
        "\n",
        "server_state = federated_algorithm.initialize()\n",
        "evaluate(server_state)\n",
        "\n",
        "NUM_ROUNDS = 15\n",
        "for round in range(NUM_ROUNDS):\n",
        "  server_state = federated_algorithm.next(server_state, federated_train_data)\n",
        "\n",
        "evaluate(server_state)\n"
      ],
      "metadata": {
        "id": "v8rGb8NlNLm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def split_image_data_realwd(data, labels, n_clients=100, verbose=True):\n",
        "#   '''\n",
        "#   Splits (data, labels) among 'n_clients s.t. every client can holds any number of classes which is trying to simulate real world dataset\n",
        "#   Input:\n",
        "#     data : [n_data x shape]\n",
        "#     labels : [n_data (x 1)] from 0 to n_labels(10)\n",
        "#     n_clients : number of clients\n",
        "#     verbose : True/False => True for printing some info, False otherwise\n",
        "#   Output:\n",
        "#     clients_split : splitted client data into desired format\n",
        "#   '''\n",
        "#   def break_into(n,m):\n",
        "#     '''\n",
        "#     return m random integers with sum equal to n\n",
        "#     '''\n",
        "#     to_ret = [1 for i in range(m)]\n",
        "#     for i in range(n-m):\n",
        "#         ind = random.randint(0,m-1)\n",
        "#         to_ret[ind] += 1\n",
        "#     return to_ret\n",
        "\n",
        "#   #### constants ####\n",
        "#   n_classes = len(set(labels))\n",
        "#   classes = list(range(n_classes))\n",
        "#   np.random.shuffle(classes)\n",
        "#   label_indcs  = [list(np.where(labels==class_)[0]) for class_ in classes]\n",
        "\n",
        "#   #### classes for each client ####\n",
        "#   tmp = [np.random.randint(1,10) for i in range(n_clients)]\n",
        "#   total_partition = sum(tmp)\n",
        "\n",
        "#   #### create partition among classes to fulfill criteria for clients ####\n",
        "#   class_partition = break_into(total_partition, len(classes))\n",
        "\n",
        "#   #### applying greedy approach first come and first serve ####\n",
        "#   class_partition = sorted(class_partition,reverse=True)\n",
        "#   class_partition_split = {}\n",
        "\n",
        "#   #### based on class partition, partitioning the label indexes ###\n",
        "#   for ind, class_ in enumerate(classes):\n",
        "#       class_partition_split[class_] = [list(i) for i in np.array_split(label_indcs[ind],class_partition[ind])]\n",
        "\n",
        "# #   print([len(class_partition_split[key]) for key in  class_partition_split.keys()])\n",
        "\n",
        "#   clients_split = []\n",
        "#   count = 0\n",
        "#   for i in range(n_clients):\n",
        "#     n = tmp[i]\n",
        "#     j = 0\n",
        "#     indcs = []\n",
        "\n",
        "#     while n>0:\n",
        "#         class_ = classes[j]\n",
        "#         if len(class_partition_split[class_])>0:\n",
        "#             indcs.extend(class_partition_split[class_][-1])\n",
        "#             count+=len(class_partition_split[class_][-1])\n",
        "#             class_partition_split[class_].pop()\n",
        "#             n-=1\n",
        "#         j+=1\n",
        "\n",
        "#     ##### sorting classes based on the number of examples it has #####\n",
        "#     classes = sorted(classes,key=lambda x:len(class_partition_split[x]),reverse=True)\n",
        "#     if n>0:\n",
        "#         raise ValueError(\" Unable to fulfill the criteria \")\n",
        "#     clients_split.append([data[indcs], labels[indcs]])\n",
        "# #   print(class_partition_split)\n",
        "# #   print(\"total example \",count)\n",
        "\n",
        "\n",
        "#   def print_split(clients_split):\n",
        "#     print(\"Data split:\")\n",
        "#     for i, client in enumerate(clients_split):\n",
        "#       split = np.sum(client[1].reshape(1,-1)==np.arange(n_labels).reshape(-1,1), axis=1)\n",
        "#       print(\" - Client {}: {}\".format(i,split))\n",
        "#     print()\n",
        "\n",
        "#     if verbose:\n",
        "#       print_split(clients_split)\n",
        "\n",
        "#   clients_split = np.array(clients_split)\n",
        "\n",
        "#   return clients_split\n",
        "\n"
      ],
      "metadata": {
        "id": "jj-dGNCIK_jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def split_image_data(data, labels, n_clients=100, classes_per_client=10, shuffle=True, verbose=True):\n",
        "#   '''\n",
        "#   Splits (data, labels) among 'n_clients s.t. every client can holds 'classes_per_client' number of classes\n",
        "#   Input:\n",
        "#     data : [n_data x shape]\n",
        "#     labels : [n_data (x 1)] from 0 to n_labels\n",
        "#     n_clients : number of clients\n",
        "#     classes_per_client : number of classes per client\n",
        "#     shuffle : True/False => True for shuffling the dataset, False otherwise\n",
        "#     verbose : True/False => True for printing some info, False otherwise\n",
        "#   Output:\n",
        "#     clients_split : client data into desired format\n",
        "#   '''\n",
        "#   #### constants ####\n",
        "#   n_data = data.shape[0]\n",
        "#   n_labels = np.max(labels) + 1\n",
        "\n",
        "\n",
        "#   ### client distribution ####\n",
        "#   data_per_client = clients_rand(len(data), n_clients)\n",
        "#   data_per_client_per_class = [np.maximum(1,nd // classes_per_client) for nd in data_per_client]\n",
        "\n",
        "#   # sort for labels\n",
        "#   data_idcs = [[] for i in range(n_labels)]\n",
        "#   for j, label in enumerate(labels):\n",
        "#     data_idcs[label] += [j]\n",
        "#   if shuffle:\n",
        "#     for idcs in data_idcs:\n",
        "#       np.random.shuffle(idcs)\n",
        "\n",
        "#   # split data among clients\n",
        "#   clients_split = []\n",
        "#   c = 0\n",
        "#   for i in range(n_clients):\n",
        "#     client_idcs = []\n",
        "\n",
        "#     budget = data_per_client[i]\n",
        "#     c = np.random.randint(n_labels)\n",
        "#     while budget > 0:\n",
        "#       take = min(data_per_client_per_class[i], len(data_idcs[c]), budget)\n",
        "\n",
        "#       client_idcs += data_idcs[c][:take]\n",
        "#       data_idcs[c] = data_idcs[c][take:]\n",
        "\n",
        "#       budget -= take\n",
        "#       c = (c + 1) % n_labels\n",
        "\n",
        "#     clients_split += [(data[client_idcs], labels[client_idcs])]\n",
        "\n",
        "#   def print_split(clients_split):\n",
        "#     print(\"Data split:\")\n",
        "#     for i, client in enumerate(clients_split):\n",
        "#       split = np.sum(client[1].reshape(1,-1)==np.arange(n_labels).reshape(-1,1), axis=1)\n",
        "#       print(\" - Client {}: {}\".format(i,split))\n",
        "#     print()\n",
        "\n",
        "#     if verbose:\n",
        "#       print_split(clients_split)\n",
        "\n",
        "#   clients_split = np.array(clients_split)\n",
        "\n",
        "#   return clients_split\n"
      ],
      "metadata": {
        "id": "Xc_IFRKrLRYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}